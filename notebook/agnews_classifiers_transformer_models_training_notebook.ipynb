{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9531094",
   "metadata": {},
   "source": [
    "# News Article Classification using Transformer Models\n",
    "\n",
    "This notebook presents the design, implementation, and evaluation of a transformer-based\n",
    "text classification system for automatically categorising news articles into four classes:\n",
    "World, Sports, Business, and Sci/Tech.\n",
    "\n",
    "The system is developed using the AG News dataset and the Hugging Face Transformers\n",
    "library. Two pretrained transformer models are considered:\n",
    "\n",
    "- DistilBERT (lightweight, efficient)\n",
    "- RoBERTa (larger, more expressive)\n",
    "\n",
    "For each model, performance is evaluated before fine-tuning (baseline) and after\n",
    "fine-tuning on the AG News dataset. Quantitative metrics and error analysis are used\n",
    "to assess model behaviour and improvements.\n",
    "\n",
    "This notebook is structured in clear sections corresponding to the project workflow:\n",
    "data preparation, modelling, evaluation, and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5040b2b",
   "metadata": {},
   "source": [
    "## 1. Environment\n",
    "\n",
    "This section sets up the Python environment used throughout the notebook.\n",
    "It imports all required libraries, configures reproducibility, and verifies\n",
    "that GPU acceleration is available for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31018ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Environment setup and verification.\n",
    "\n",
    "This cell:\n",
    "- Imports all core libraries used in the project\n",
    "- Sets random seeds for reproducibility\n",
    "- Verifies availability of a CUDA-enabled GPU\n",
    "\"\"\"\n",
    "\n",
    "# Core numerical and data libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Hugging Face datasets and transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# Evaluation and metrics\n",
    "import evaluate\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Reproducibility\n",
    "# -------------------------------------------------------------------\n",
    "set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# GPU verification\n",
    "# -------------------------------------------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU available: {device_name}\")\n",
    "else:\n",
    "    raise RuntimeError(\"CUDA-compatible GPU not detected. Check PyTorch installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07d2f3",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading\n",
    "\n",
    "This section loads the AG News dataset from the Hugging Face Datasets library.\n",
    "The dataset consists of news article titles and short descriptions, each labelled\n",
    "into one of four categories: World, Sports, Business, or Sci/Tech.\n",
    "\n",
    "The dataset structure and a representative training example are inspected to\n",
    "verify correct loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5979c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load and inspect the AG News dataset.\n",
    "\n",
    "This cell:\n",
    "- Loads the AG News dataset from Hugging Face\n",
    "- Displays the dataset structure\n",
    "- Prints a representative training example\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the AG News dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Display dataset structure\n",
    "print(dataset)\n",
    "\n",
    "# Display a representative training example\n",
    "print(\"\\nExample training sample:\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea5ef6",
   "metadata": {},
   "source": [
    "## 3. Dataset Overview and Class Distribution\n",
    "\n",
    "This section provides an overview of the label structure of the AG News dataset\n",
    "and examines the distribution of samples across the four news categories in both\n",
    "the training and test splits.\n",
    "\n",
    "Understanding class balance is important to ensure that model evaluation metrics\n",
    "are meaningful and not biased toward overrepresented categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d881996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset overview and class distribution analysis.\n",
    "\n",
    "This cell:\n",
    "- Maps numerical labels to human-readable class names\n",
    "- Computes class counts for training and test splits\n",
    "- Visualises class distributions using bar charts\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "label_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "train_df[\"label_name\"] = train_df[\"label\"].map(lambda x: label_names[x])\n",
    "test_df[\"label_name\"] = test_df[\"label\"].map(lambda x: label_names[x])\n",
    "\n",
    "train_counts = train_df[\"label_name\"].value_counts().sort_index()\n",
    "test_counts = test_df[\"label_name\"].value_counts().sort_index()\n",
    "\n",
    "print(\"Training set class distribution:\")\n",
    "print(train_counts)\n",
    "print(\"\\nTest set class distribution:\")\n",
    "print(test_counts)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "colors = [\"#0173B2\", \"#DE8F05\"]\n",
    "\n",
    "for ax, counts, title, color in zip(\n",
    "    axes,\n",
    "    [train_counts, test_counts],\n",
    "    [\"Training Set\", \"Test Set\"],\n",
    "    colors\n",
    "):\n",
    "    bars = ax.bar(counts.index, counts.values, color=color, alpha=0.8, edgecolor=\"none\")\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2, height,\n",
    "            f\"{int(height)}\",\n",
    "            ha=\"center\", va=\"bottom\", fontsize=12, fontweight=\"150\"\n",
    "        )\n",
    "    \n",
    "    ax.set_title(f\"{title} Class Distribution\", fontsize=16, fontweight=\"200\", pad=15)\n",
    "    ax.set_xlabel(\"Category\", fontsize=14, fontweight=\"150\")\n",
    "    ax.set_ylabel(\"Number of Articles\", fontsize=14, fontweight=\"150\")\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.tick_params(axis=\"both\", labelsize=12)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(\"#e0e0e0\")\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fca06",
   "metadata": {},
   "source": [
    "## 4. Text Length Analysis\n",
    "\n",
    "This section analyses the length of news article texts in the training data.\n",
    "The goal is to understand typical input sizes and determine an appropriate\n",
    "maximum sequence length for transformer tokenisation.\n",
    "\n",
    "Summary statistics and a histogram of text lengths are used to support this decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "train_df[\"text_length\"] = train_df[\"text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Text length statistics (training set):\")\n",
    "print(train_df[\"text_length\"].describe())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "sns.histplot(\n",
    "    train_df[\"text_length\"],\n",
    "    bins=50,\n",
    "    kde=True,\n",
    "    stat=\"count\",\n",
    "    color=\"#0173B2\",\n",
    "    alpha=0.75,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "mean_val = train_df[\"text_length\"].mean()\n",
    "ax.axvline(\n",
    "    mean_val,\n",
    "    color=\"#DE8F05\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean = {mean_val:.1f} words\",\n",
    "    zorder=5\n",
    ")\n",
    "\n",
    "ax.set_title(\"Distribution of News Article Text Lengths\", fontsize=15, fontweight=\"200\", pad=18)\n",
    "ax.set_xlabel(\"Number of Words per Article\", fontsize=12, fontweight=\"150\")\n",
    "ax.set_ylabel(\"Number of Articles\", fontsize=12, fontweight=\"150\")\n",
    "ax.legend(fontsize=11, loc=\"upper right\", framealpha=0.95)\n",
    "ax.grid(True, alpha=0.25, linestyle=\"--\", linewidth=0.8)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor(\"#e0e0e0\")\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tick_params(axis=\"both\", labelsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac67716",
   "metadata": {},
   "source": [
    "## 5. Subsampling and Dataset Preparation\n",
    "\n",
    "To ensure efficient training within local hardware constraints while preserving\n",
    "class balance, a smaller stratified subset of the dataset is created.\n",
    "\n",
    "This section constructs:\n",
    "- A balanced training subset of 8,000 samples (2000 per class)\n",
    "- A balanced test subset of 2,000 samples (500 per class)\n",
    "\n",
    "The subsets are then converted back into Hugging Face Dataset objects for\n",
    "subsequent tokenisation and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc330a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Balanced subsampling and dataset preparation.\n",
    "\n",
    "This cell:\n",
    "- Creates stratified training and test subsets\n",
    "- Verifies class balance\n",
    "- Converts subsets back to Hugging Face Dataset format\n",
    "\"\"\"\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Stratified subsampling\n",
    "train_subset = train_df.groupby(\"label_name\", group_keys=False).sample(\n",
    "    n=2000, random_state=42\n",
    ")\n",
    "\n",
    "test_subset = test_df.groupby(\"label_name\", group_keys=False).sample(\n",
    "    n=500, random_state=42\n",
    ")\n",
    "\n",
    "# Verify class balance\n",
    "print(\"Subsampled training set distribution:\")\n",
    "print(train_subset[\"label_name\"].value_counts())\n",
    "\n",
    "print(\"\\nSubsampled test set distribution:\")\n",
    "print(test_subset[\"label_name\"].value_counts())\n",
    "\n",
    "# Convert back to Hugging Face Dataset format\n",
    "train_hf = Dataset.from_pandas(train_subset.reset_index(drop=True))\n",
    "test_hf = Dataset.from_pandas(test_subset.reset_index(drop=True))\n",
    "\n",
    "mini_dataset = DatasetDict({\n",
    "    \"train\": train_hf,\n",
    "    \"test\": test_hf\n",
    "})\n",
    "\n",
    "mini_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f032094",
   "metadata": {},
   "source": [
    "## 6. Tokenisation Strategy\n",
    "\n",
    "Tokenisation is a critical step in transformer-based text classification, as it\n",
    "determines how raw text is converted into numerical representations suitable for\n",
    "model input. The choice of maximum sequence length directly affects both model\n",
    "performance and computational efficiency.\n",
    "\n",
    "In this project, tokenisation decisions are informed by empirical analysis rather\n",
    "than arbitrary defaults. First, a token length truncation analysis is conducted\n",
    "for each selected model to quantify how many samples would be affected by common\n",
    "maximum sequence lengths. Based on these findings, an appropriate maximum length\n",
    "is selected and applied consistently during dataset preparation.\n",
    "\n",
    "This section is divided into two subsections:\n",
    "\n",
    "- **Section 6.1:** Token length truncation analysis for DistilBERT and RoBERTa  \n",
    "- **Section 6.2:** Tokenisation of the dataset using the selected maximum length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15998e59",
   "metadata": {},
   "source": [
    "### 6.1. Token Length Truncation Analysis\n",
    "\n",
    "Before tokenising the dataset for model training, it is important to analyse how\n",
    "different maximum sequence lengths affect truncation. Transformer models have a\n",
    "fixed maximum input length, and overly small limits may truncate important\n",
    "information, while overly large limits increase computational cost unnecessarily.\n",
    "\n",
    "This section analyses token length distributions using the tokenisers associated\n",
    "with both selected models:\n",
    "- DistilBERT\n",
    "- RoBERTa\n",
    "\n",
    "For each model, the proportion of samples exceeding common maximum lengths\n",
    "(128 and 256 tokens) is computed and reported in tabular form. This analysis\n",
    "informs the choice of an appropriate maximum sequence length for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Token length truncation analysis for DistilBERT and RoBERTa.\n",
    "\n",
    "This cell:\n",
    "- Loads the tokenisers for both models\n",
    "- Computes true token lengths without truncation\n",
    "- Calculates how many samples exceed common max lengths\n",
    "- Presents the results in a clear table\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Models to analyse\n",
    "models = {\n",
    "    \"DistilBERT\": \"distilbert-base-uncased\",\n",
    "    \"RoBERTa\": \"roberta-base\"\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Analyse token lengths for each model\n",
    "for model_name, model_checkpoint in models.items():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Tokenise without truncation to get true token lengths\n",
    "    token_lengths = [\n",
    "        len(tokenizer.encode(text, add_special_tokens=True))\n",
    "        for text in train_df[\"text\"]\n",
    "    ]\n",
    "\n",
    "    token_lengths = np.array(token_lengths)\n",
    "    total_samples = len(token_lengths)\n",
    "\n",
    "    # Truncation counts\n",
    "    trunc_100 = np.sum(token_lengths > 100)\n",
    "    trunc_128 = np.sum(token_lengths > 128)\n",
    "    trunc_256 = np.sum(token_lengths > 256)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Total Samples\": total_samples,\n",
    "        \"Samples > 100 tokens\": trunc_100,\n",
    "        \"Percent > 100 tokens\": trunc_100 / total_samples,\n",
    "        \"Samples > 128 tokens\": trunc_128,\n",
    "        \"Percent > 128 tokens\": trunc_128 / total_samples,\n",
    "        \"Samples > 256 tokens\": trunc_256,\n",
    "        \"Percent > 256 tokens\": trunc_256 / total_samples,\n",
    "    })\n",
    "\n",
    "# Create results table\n",
    "truncation_df = pd.DataFrame(results)\n",
    "\n",
    "# Format percentages for readability\n",
    "truncation_df[\"Percent > 100 tokens\"] = truncation_df[\"Percent > 100 tokens\"].map(lambda x: f\"{x:.2%}\")\n",
    "truncation_df[\"Percent > 128 tokens\"] = truncation_df[\"Percent > 128 tokens\"].map(lambda x: f\"{x:.2%}\")\n",
    "truncation_df[\"Percent > 256 tokens\"] = truncation_df[\"Percent > 256 tokens\"].map(lambda x: f\"{x:.2%}\")\n",
    "\n",
    "truncation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb511f",
   "metadata": {},
   "source": [
    "### 6.2 Dataset Tokenisation\n",
    "\n",
    "In this subsection, the balanced dataset is tokenised using the pretrained\n",
    "tokenisers corresponding to the selected transformer models: DistilBERT and\n",
    "RoBERTa. Based on the truncation analysis in Section 6.1, a maximum sequence\n",
    "length of 128 tokens is used for both models.\n",
    "\n",
    "Tokenisation converts raw text into numerical inputs required by transformer\n",
    "models, including input IDs and attention masks. The tokenised datasets are\n",
    "prepared in a format compatible with PyTorch and the Hugging Face Trainer API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset tokenisation for DistilBERT and RoBERTa.\n",
    "\n",
    "This cell:\n",
    "- Loads the appropriate tokenisers for each model\n",
    "- Tokenises the balanced dataset using a fixed maximum sequence length\n",
    "- Prepares tokenised datasets for PyTorch-based training\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Model checkpoints\n",
    "model_checkpoints = {\n",
    "    \"distilbert\": \"distilbert-base-uncased\",\n",
    "    \"roberta\": \"roberta-base\"\n",
    "}\n",
    "\n",
    "tokenized_datasets = {}\n",
    "\n",
    "for model_name, checkpoint in model_checkpoints.items():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    def tokenize_function(batch):\n",
    "        \"\"\"\n",
    "        Tokenise a batch of text examples.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): A batch containing the 'text' field.\n",
    "\n",
    "        Returns:\n",
    "            dict: Tokenised outputs including input_ids and attention_mask.\n",
    "        \"\"\"\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "\n",
    "    # Apply tokenisation\n",
    "    tokenized_dataset = mini_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\", \"label_name\"]\n",
    "    )\n",
    "\n",
    "    # Set format for PyTorch\n",
    "    tokenized_dataset.set_format(\n",
    "        type=\"torch\",\n",
    "        columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    "    )\n",
    "\n",
    "    tokenized_datasets[model_name] = tokenized_dataset\n",
    "\n",
    "    \n",
    "# Inspect tokenised dataset structures for verification\n",
    "print(\"DistilBERT tokenised dataset:\")\n",
    "print(tokenized_datasets[\"distilbert\"])\n",
    "\n",
    "print(\"\\nRoBERTa tokenised dataset:\")\n",
    "print(tokenized_datasets[\"roberta\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create visual of end-to-end work flow\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(14, 3))\n",
    "fig.patch.set_facecolor(\"#f4f4f9\")\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 2)\n",
    "\n",
    "# Define color palette for each step\n",
    "COLORS = [\"#003f5c\", \"#58508d\", \"#bc5090\", \"#ff6361\", \"#ffa600\"]\n",
    "TEXT_COLOR = \"white\"\n",
    "ARROW_COLOR = \"#555555\"\n",
    "\n",
    "# Pipeline labels\n",
    "labels = [\n",
    "    \"Raw Text\",\n",
    "    \"Tokeniser\",\n",
    "    \"Transformer\\nEncoder\",\n",
    "    \"Classification\\nHead\",\n",
    "    \"Predicted\\nCategory\"\n",
    "]\n",
    "\n",
    "# Box positions on x-axis\n",
    "positions = [1, 4, 7, 10, 13]\n",
    "\n",
    "# Draw boxes\n",
    "for x, label, color in zip(positions, labels, COLORS):\n",
    "    box = patches.FancyBboxPatch(\n",
    "        (x - 1, 0.7), 2, 0.8,\n",
    "        boxstyle=\"round,pad=0.1,rounding_size=0.2\",\n",
    "        linewidth=0,\n",
    "        facecolor=color\n",
    "    )\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, 1.1, label, ha='center', va='center', fontsize=11, color=TEXT_COLOR, fontweight='250')\n",
    "\n",
    "# Draw arrows between boxes\n",
    "for i in range(len(positions) - 1):\n",
    "    ax.annotate(\n",
    "        \"\", \n",
    "        xy=(positions[i+1]-1, 1.1), \n",
    "        xytext=(positions[i]+1, 1.1),\n",
    "        arrowprops=dict(arrowstyle=\"-|>\", lw=2.5, color=ARROW_COLOR)\n",
    "    )\n",
    "\n",
    "ax.axis(\"off\")\n",
    "plt.title(\"Transformer-Based News Classification Pipeline\", fontsize=14, pad=20, fontweight='400', color=\"#333333\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d6045",
   "metadata": {},
   "source": [
    "## 7. Baseline Model Evaluation (Pre-Fine-Tuning)\n",
    "\n",
    "Before fine-tuning, each pretrained transformer model is evaluated on the test\n",
    "dataset to establish a baseline performance. This baseline reflects how well\n",
    "the model performs on the news classification task without any task-specific\n",
    "training.\n",
    "\n",
    "Evaluating the baseline is important for quantifying the improvement gained\n",
    "through fine-tuning and for understanding the extent to which pretrained\n",
    "language representations alone capture task-relevant information.\n",
    "\n",
    "In this section, both DistilBERT and RoBERTa are evaluated using the same test\n",
    "set and the same evaluation metrics to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ec477",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Baseline (pre-fine-tuning) evaluation for DistilBERT and RoBERTa.\n",
    "\n",
    "This cell:\n",
    "- Loads pretrained models without fine-tuning\n",
    "- Evaluates them on the test split\n",
    "- Computes accuracy, macro-averaged precision, recall, and F1-score\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for classification.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): Tuple containing logits and true labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "    }\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "for model_name, checkpoint in model_checkpoints.items():\n",
    "    # Load pretrained model (no fine-tuning)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        num_labels=4\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=AutoTokenizer.from_pretrained(checkpoint),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    metrics = trainer.evaluate(tokenized_datasets[model_name][\"test\"])\n",
    "\n",
    "    # Store results\n",
    "    metrics[\"model\"] = model_name\n",
    "    baseline_results.append(metrics)\n",
    "\n",
    "# Convert results to DataFrame for clarity\n",
    "baseline_df = pd.DataFrame(baseline_results).set_index(\"model\")\n",
    "\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843d77c",
   "metadata": {},
   "source": [
    "## 8. Supervised Fine-Tuning Strategies\n",
    "\n",
    "In this section, supervised fine-tuning is performed using two different strategies\n",
    "for each selected transformer model:\n",
    "\n",
    "1. **Full Fine-Tuning**  \n",
    "   All model parameters are updated during training.\n",
    "\n",
    "2. **Low-Rank Adaptation (LoRA)**  \n",
    "   The base model parameters are frozen, and only a small number of trainable\n",
    "   low-rank adapter parameters are introduced and trained.\n",
    "\n",
    "Both strategies are applied to DistilBERT and RoBERTa using identical training\n",
    "hyperparameters. This controlled setup enables a fair comparison between:\n",
    "\n",
    "- Pretrained baseline performance (Section 7)\n",
    "- Full supervised fine-tuning\n",
    "- Parameter-efficient fine-tuning via LoRA\n",
    "\n",
    "The best-performing checkpoint from each fine-tuning strategy is saved for\n",
    "subsequent evaluation and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382a171",
   "metadata": {},
   "source": [
    "### 8.1 DistilBERT Fine-Tuning\n",
    "\n",
    "This subsection fine-tunes DistilBERT using full fine-tuning and LoRA-based\n",
    "parameter-efficient fine-tuning. Training metrics and best checkpoints are saved\n",
    "for subsequent evaluation and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf806de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Supervised fine-tuning of DistilBERT with and without LoRA.\n",
    "\n",
    "This cell:\n",
    "- Performs full fine-tuning of DistilBERT\n",
    "- Performs LoRA-based fine-tuning of DistilBERT\n",
    "- Saves best checkpoints and training metrics to disk\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for classification.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): Tuple containing logits and true labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------\n",
    "# training hyperparameters\n",
    "# --------------------------------------------------\n",
    "common_args = dict(\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=4,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,        \n",
    "    save_only_model=True,       \n",
    "    logging_strategy=\"epoch\",  \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Full fine-tuning\n",
    "# --------------------------------------------------\n",
    "distilbert_full = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "full_args = TrainingArguments(\n",
    "    output_dir=\"models/distilbert_full\",\n",
    "    **common_args\n",
    ")\n",
    "\n",
    "trainer_full = Trainer(\n",
    "    model=distilbert_full,\n",
    "    args=full_args,\n",
    "    train_dataset=tokenized_datasets[\"distilbert\"][\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"distilbert\"][\"test\"],\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_result_full = trainer_full.train()\n",
    "trainer_full.save_model(\"models/distilbert_full/best_model\")\n",
    "\n",
    "# Save training metrics\n",
    "with open(\"models/distilbert_full/train_metrics.json\", \"w\") as f:\n",
    "    json.dump(train_result_full.metrics, f, indent=2)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# LoRA fine-tuning (DistilBERT-specific configuration)\n",
    "# ----------------------------------------------------\n",
    "distilbert_lora = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "distilbert_lora = get_peft_model(distilbert_lora, lora_config)\n",
    "\n",
    "lora_args = TrainingArguments(\n",
    "    output_dir=\"models/distilbert_lora\",\n",
    "    **common_args\n",
    ")\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=distilbert_lora,\n",
    "    args=lora_args,\n",
    "    train_dataset=tokenized_datasets[\"distilbert\"][\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"distilbert\"][\"test\"],\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased\"),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_result_lora = trainer_lora.train()\n",
    "trainer_lora.save_model(\"models/distilbert_lora/best_model\")\n",
    "\n",
    "# Save training metrics\n",
    "with open(\"models/distilbert_lora/train_metrics.json\", \"w\") as f:\n",
    "    json.dump(train_result_lora.metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0138cf",
   "metadata": {},
   "source": [
    "### 8.2 RoBERTa Fine-Tuning\n",
    "\n",
    "This subsection fine-tunes RoBERTa using full fine-tuning and LoRA-based\n",
    "parameter-efficient fine-tuning. Training metrics and best checkpoints are saved\n",
    "for direct comparison with DistilBERT and baseline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828113af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Supervised fine-tuning of RoBERTa with and without LoRA.\n",
    "\n",
    "This cell:\n",
    "- Performs full fine-tuning of RoBERTa\n",
    "- Performs LoRA-based fine-tuning of RoBERTa\n",
    "- Saves best checkpoints and training metrics to disk\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for classification.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): Tuple containing logits and true labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------\n",
    "# training hyperparameters\n",
    "# --------------------------------------------------\n",
    "common_args = dict(\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=4,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=1,        \n",
    "    save_only_model=True,       \n",
    "    logging_strategy=\"epoch\",  \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Full fine-tuning\n",
    "# --------------------------------------------------\n",
    "roberta_full = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "full_args = TrainingArguments(\n",
    "    output_dir=\"models/roberta_full\",\n",
    "    **common_args\n",
    ")\n",
    "\n",
    "trainer_full = Trainer(\n",
    "    model=roberta_full,\n",
    "    args=full_args,\n",
    "    train_dataset=tokenized_datasets[\"roberta\"][\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"roberta\"][\"test\"],\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"roberta-base\"),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_result_full = trainer_full.train()\n",
    "trainer_full.save_model(\"models/roberta_full/best_model\")\n",
    "\n",
    "with open(\"models/roberta_full/train_metrics.json\", \"w\") as f:\n",
    "    json.dump(train_result_full.metrics, f, indent=2)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# LoRA fine-tuning (RoBERTa-specific configuration)\n",
    "# --------------------------------------------------\n",
    "roberta_lora = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "lora_config_roberta = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "roberta_lora = get_peft_model(roberta_lora, lora_config_roberta)\n",
    "\n",
    "lora_args = TrainingArguments(\n",
    "    output_dir=\"models/roberta_lora\",\n",
    "    **common_args\n",
    ")\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=roberta_lora,\n",
    "    args=lora_args,\n",
    "    train_dataset=tokenized_datasets[\"roberta\"][\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"roberta\"][\"test\"],\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"roberta-base\"),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_result_lora = trainer_lora.train()\n",
    "trainer_lora.save_model(\"models/roberta_lora/best_model\")\n",
    "\n",
    "with open(\"models/roberta_lora/train_metrics.json\", \"w\") as f:\n",
    "    json.dump(train_result_lora.metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89501014",
   "metadata": {},
   "source": [
    "## 9. Ensemble Model Evaluation\n",
    "\n",
    "While individual fine-tuned models already demonstrate substantial performance\n",
    "improvements over the pretrained baseline, ensemble methods can further improve\n",
    "robustness and generalisation by combining complementary model predictions.\n",
    "\n",
    "In this section, probability-level ensembling is applied to the four fine-tuned\n",
    "models produced in Section 8:\n",
    "\n",
    "- DistilBERT (full fine-tuning)\n",
    "- DistilBERT (LoRA)\n",
    "- RoBERTa (full fine-tuning)\n",
    "- RoBERTa (LoRA)\n",
    "\n",
    "Predicted class probabilities are averaged using weighted combinations, and\n",
    "ensemble performance is evaluated using accuracy, macro-averaged precision,\n",
    "recall, and F1-score. The objective is to assess whether ensembles outperform\n",
    "individual models and to identify which model combinations contribute most\n",
    "effectively to performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36414702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensemble evaluation using probability-level model combination.\n",
    "\n",
    "This cell:\n",
    "- Loads the best fine-tuned checkpoints from disk\n",
    "- Computes class probability distributions on the test set\n",
    "- Evaluates all model combinations using weighted averaging\n",
    "- Identifies the best-performing ensemble based on macro F1-score\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from torch.nn.functional import softmax\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Device configuration\n",
    "# --------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load best fine-tuned models\n",
    "# --------------------------------------------------\n",
    "ensemble_models = {\n",
    "    \"DistilBERT_Full\": \"models/distilbert_full/best_model\",\n",
    "    \"DistilBERT_LoRA\": \"models/distilbert_lora/best_model\",\n",
    "    \"RoBERTa_Full\": \"models/roberta_full/best_model\",\n",
    "    \"RoBERTa_LoRA\": \"models/roberta_lora/best_model\",\n",
    "}\n",
    "\n",
    "loaded_models = {}\n",
    "\n",
    "for name, path in ensemble_models.items():\n",
    "\n",
    "    if \"LoRA\" in name:\n",
    "        # Load base model first\n",
    "        if \"DistilBERT\" in name:\n",
    "            base_checkpoint = \"distilbert-base-uncased\"\n",
    "        else:\n",
    "            base_checkpoint = \"roberta-base\"\n",
    "\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            base_checkpoint,\n",
    "            num_labels=4\n",
    "        )\n",
    "\n",
    "        model = PeftModel.from_pretrained(base_model, path)\n",
    "\n",
    "    else:\n",
    "        # Full fine-tuned model\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    loaded_models[name] = model\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Compute probability distributions for each model\n",
    "# --------------------------------------------------\n",
    "def get_model_probabilities(model, dataset, batch_size=32):\n",
    "    \"\"\"\n",
    "    Compute predicted class probabilities for an entire dataset.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): Fine-tuned classification model.\n",
    "        dataset (datasets.Dataset): Tokenised dataset split.\n",
    "        batch_size (int): Batch size for inference.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N, num_classes) with class probabilities.\n",
    "    \"\"\"\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, pin_memory=True\n",
    "    )\n",
    "\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            ).logits\n",
    "\n",
    "            probs = softmax(logits, dim=-1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "# Compute probabilities for each model\n",
    "model_probabilities = {\n",
    "    name: get_model_probabilities(model, tokenized_datasets[\n",
    "        \"distilbert\" if \"DistilBERT\" in name else \"roberta\"\n",
    "    ][\"test\"])\n",
    "    for name, model in loaded_models.items()\n",
    "}\n",
    "\n",
    "true_labels = np.array(mini_dataset[\"test\"][\"label\"])\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Ensemble evaluation\n",
    "# --------------------------------------------------\n",
    "def evaluate_ensemble(model_subset, weights):\n",
    "    \"\"\"\n",
    "    Evaluate a weighted ensemble of models.\n",
    "\n",
    "    Args:\n",
    "        model_subset (tuple[str]): Selected model names.\n",
    "        weights (np.ndarray): Normalised ensemble weights.\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics.\n",
    "    \"\"\"\n",
    "    combined_prob = np.zeros_like(model_probabilities[model_subset[0]])\n",
    "\n",
    "    for model_name, w in zip(model_subset, weights):\n",
    "        combined_prob += w * model_probabilities[model_name]\n",
    "\n",
    "    preds = combined_prob.argmax(axis=1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, preds),\n",
    "        \"precision_macro\": precision_score(true_labels, preds, average=\"macro\"),\n",
    "        \"recall_macro\": recall_score(true_labels, preds, average=\"macro\"),\n",
    "        \"f1_macro\": f1_score(true_labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Search over ensemble combinations\n",
    "# --------------------------------------------------\n",
    "model_names = list(model_probabilities.keys())\n",
    "\n",
    "combinations_list = (\n",
    "    list(itertools.combinations(model_names, 2)) +\n",
    "    list(itertools.combinations(model_names, 3)) +\n",
    "    [tuple(model_names)]\n",
    ")\n",
    "\n",
    "weight_grid = np.arange(0.1, 1.01, 0.1)\n",
    "\n",
    "ensemble_results = []\n",
    "\n",
    "for subset in combinations_list:\n",
    "    best_f1 = -1\n",
    "    best_weights = None\n",
    "    best_metrics = None\n",
    "\n",
    "    for weights in itertools.product(weight_grid, repeat=len(subset)):\n",
    "        weights = np.array(weights)\n",
    "        if weights.sum() == 0 or weights.min() < 0.05:\n",
    "            continue\n",
    "\n",
    "        weights = weights / weights.sum()\n",
    "        metrics = evaluate_ensemble(subset, weights)\n",
    "\n",
    "        if metrics[\"f1_macro\"] > best_f1:\n",
    "            best_f1 = metrics[\"f1_macro\"]\n",
    "            best_weights = weights\n",
    "            best_metrics = metrics\n",
    "\n",
    "    ensemble_results.append({\n",
    "        \"models\": subset,\n",
    "        \"weights\": best_weights,\n",
    "        **best_metrics\n",
    "    })\n",
    "\n",
    "ensemble_df = pd.DataFrame(ensemble_results)\n",
    "ensemble_df.sort_values(\"f1_macro\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b5f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Persist ensemble evaluation results to disk.\n",
    "\n",
    "This cell:\n",
    "- Saves the full ensemble results table\n",
    "- Saves the best-performing ensemble configuration\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure results directory exists\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save full ensemble results\n",
    "ensemble_df.to_csv(results_dir / \"ensemble_results.csv\", index=False)\n",
    "ensemble_df.to_json(\n",
    "    results_dir / \"ensemble_results.json\",\n",
    "    orient=\"records\",\n",
    "    indent=2\n",
    ")\n",
    "\n",
    "# Extract and save best ensemble only\n",
    "best_ensemble = ensemble_df.sort_values(\n",
    "    \"f1_macro\", ascending=False\n",
    ").iloc[0]\n",
    "\n",
    "best_ensemble.to_json(\n",
    "    results_dir / \"best_ensemble.json\",\n",
    "    indent=2\n",
    ")\n",
    "\n",
    "print(\"Ensemble results saved to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46737cdf",
   "metadata": {},
   "source": [
    "## 10. Fine-Tuned Model Evaluation and Error Analysis\n",
    "\n",
    "This section evaluates and compares the performance of all fine-tuned models\n",
    "and the best-performing ensemble. Quantitative metrics and diagnostic analyses\n",
    "are used to assess predictive quality, robustness, and error characteristics.\n",
    "\n",
    "The analysis focuses on three levels of comparison:\n",
    "- Individual fine-tuned models\n",
    "- Selection of the best single model\n",
    "- Performance gains achieved through ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a56553",
   "metadata": {},
   "source": [
    "### 10.1 Evaluation of Fine-Tuned Models\n",
    "\n",
    "This subsection evaluates all fine-tuned models using their respective best\n",
    "checkpoints saved during training. Performance is assessed on the test set\n",
    "using consistent evaluation metrics, enabling direct comparison between:\n",
    "\n",
    "- DistilBERT with full fine-tuning\n",
    "- DistilBERT with LoRA fine-tuning\n",
    "- RoBERTa with full fine-tuning\n",
    "- RoBERTa with LoRA fine-tuning\n",
    "\n",
    "These results form the basis for selecting the best-performing single model\n",
    "in the subsequent subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation of best fine-tuned checkpoints.\n",
    "\n",
    "This cell:\n",
    "- Loads best saved checkpoints for all fine-tuned models\n",
    "- Evaluates them on the test set\n",
    "- Produces a unified comparison table of evaluation metrics\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for classification.\n",
    "\n",
    "    Args:\n",
    "        eval_pred (tuple): Tuple containing logits and true labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "    }\n",
    "\n",
    "fine_tuned_checkpoints = {\n",
    "    \"DistilBERT_Full\": {\n",
    "        \"path\": \"models/distilbert_full/best_model\",\n",
    "        \"base\": \"distilbert-base-uncased\",\n",
    "        \"type\": \"full\",\n",
    "        \"dataset\": \"distilbert\",\n",
    "    },\n",
    "    \"DistilBERT_LoRA\": {\n",
    "        \"path\": \"models/distilbert_lora/best_model\",\n",
    "        \"base\": \"distilbert-base-uncased\",\n",
    "        \"type\": \"lora\",\n",
    "        \"dataset\": \"distilbert\",\n",
    "    },\n",
    "    \"RoBERTa_Full\": {\n",
    "        \"path\": \"models/roberta_full/best_model\",\n",
    "        \"base\": \"roberta-base\",\n",
    "        \"type\": \"full\",\n",
    "        \"dataset\": \"roberta\",\n",
    "    },\n",
    "    \"RoBERTa_LoRA\": {\n",
    "        \"path\": \"models/roberta_lora/best_model\",\n",
    "        \"base\": \"roberta-base\",\n",
    "        \"type\": \"lora\",\n",
    "        \"dataset\": \"roberta\",\n",
    "    },\n",
    "}\n",
    "\n",
    "evaluation_rows = []\n",
    "\n",
    "for model_name, cfg in fine_tuned_checkpoints.items():\n",
    "\n",
    "    if cfg[\"type\"] == \"lora\":\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            cfg[\"base\"],\n",
    "            num_labels=4\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base_model, cfg[\"path\"])\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(cfg[\"path\"])\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    metrics = trainer.evaluate(\n",
    "        tokenized_datasets[cfg[\"dataset\"]][\"test\"]\n",
    "    )\n",
    "\n",
    "    evaluation_rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"eval_accuracy\": metrics[\"eval_accuracy\"],\n",
    "        \"eval_precision_macro\": metrics[\"eval_precision_macro\"],\n",
    "        \"eval_recall_macro\": metrics[\"eval_recall_macro\"],\n",
    "        \"eval_f1_macro\": metrics[\"eval_f1_macro\"],\n",
    "    })\n",
    "\n",
    "fine_tuned_eval_df = (\n",
    "    pd.DataFrame(evaluation_rows)\n",
    "    .set_index(\"Model\")\n",
    "    .sort_values(\"eval_f1_macro\", ascending=False)\n",
    ")\n",
    "\n",
    "fine_tuned_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1d815",
   "metadata": {},
   "source": [
    "### 10.2 Ensemble Performance Evaluation\n",
    "\n",
    "This subsection evaluates the performance of ensemble models constructed from\n",
    "the fine-tuned transformers. The ensemble combines model predictions at the\n",
    "probability level using weighted averaging.\n",
    "\n",
    "Performance is reported using the same evaluation metrics as individual models\n",
    "to allow direct comparison between single-model fine-tuning and ensemble-based\n",
    "approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation of ensemble model performance.\n",
    "\n",
    "This cell:\n",
    "- Loads previously computed ensemble results from disk\n",
    "- Identifies and present the best-performing ensemble configuration\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load ensemble evaluation results\n",
    "ensemble_results = pd.read_csv(\"results/ensemble_results.csv\")\n",
    "\n",
    "# Sort by macro F1-score\n",
    "ensemble_results_sorted = ensemble_results.sort_values(\n",
    "    \"f1_macro\", ascending=False\n",
    ")\n",
    "\n",
    "# Display the best-performing ensemble configurations\n",
    "best_ensemble = ensemble_results_sorted.iloc[0]\n",
    "best_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be896d2c",
   "metadata": {},
   "source": [
    "### 10.3 Error Analysis and Confusion Matrix Evaluation\n",
    "\n",
    "This subsection performs detailed error analysis for all fine-tuned models and\n",
    "the ensemble approach. Confusion matrices are used to visualise class-level\n",
    "performance and systematic misclassification patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate predictions on the test set for all fine-tuned models and the ensemble.\n",
    "\n",
    "This cell:\n",
    "- Runs inference for each fine-tuned model\n",
    "- Stores true labels and predicted labels\n",
    "- Prepares predictions for confusion matrix analysis\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "# True labels\n",
    "y_true = np.array(mini_dataset[\"test\"][\"label\"])\n",
    "\n",
    "model_predictions = {}\n",
    "\n",
    "# Generate predictions for individual models\n",
    "for model_name, model in loaded_models.items():\n",
    "\n",
    "    dataset_key = \"distilbert\" if \"DistilBERT\" in model_name else \"roberta\"\n",
    "    dataset = tokenized_datasets[dataset_key][\"test\"]\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=32, shuffle=False\n",
    "    )\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            ).logits\n",
    "\n",
    "            preds.append(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "\n",
    "    model_predictions[model_name] = np.concatenate(preds)\n",
    "\n",
    "# Ensemble predictions (best ensemble configuration)\n",
    "best_ensemble_config = [\n",
    "    m.strip().strip(\"'\")\n",
    "    for m in best_ensemble[\"models\"].strip(\"()\").split(\",\")\n",
    "]\n",
    "\n",
    "best_weights = np.fromstring(\n",
    "    best_ensemble[\"weights\"].strip(\"[]\"),\n",
    "    sep=\" \"\n",
    ")\n",
    "\n",
    "ensemble_probs = np.zeros_like(\n",
    "    model_probabilities[best_ensemble_config[0]]\n",
    ")\n",
    "\n",
    "for model_name, w in zip(best_ensemble_config, best_weights):\n",
    "    ensemble_probs += w * model_probabilities[model_name]\n",
    "\n",
    "model_predictions[\"Ensemble\"] = ensemble_probs.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot confusion matrices for DistilBERT and RoBERTa fine-tuning strategies.\n",
    "\n",
    "This cell:\n",
    "- Plots Full vs LoRA confusion matrices side by side\n",
    "- Saves figures as PNG files\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "label_order = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "results_dir = Path(\"results/confusion_matrices\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set clean style\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "\n",
    "def plot_clean_confusion_matrix(cm, ax, title, cmap='Blues'):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix with counts and percentages.\n",
    "    \"\"\"\n",
    "    # Calculate percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create annotations combining counts and percentages\n",
    "    annotations = np.empty_like(cm, dtype=object)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            count = cm[i, j]\n",
    "            percent = cm_percent[i, j]\n",
    "            annotations[i, j] = f'{count}\\n({percent:.1f}%)'\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=annotations,\n",
    "        fmt='',\n",
    "        cmap=cmap,\n",
    "        xticklabels=label_order,\n",
    "        yticklabels=label_order,\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'Count'},\n",
    "        linewidths=1.5,\n",
    "        linecolor='white',\n",
    "        square=True,\n",
    "        vmin=0,\n",
    "        annot_kws={'fontsize': 11, 'weight': 'bold'},\n",
    "        cbar=True\n",
    "    )\n",
    "    \n",
    "    # Title styling\n",
    "    ax.set_title(title, fontsize=15, fontweight='300', pad=12)\n",
    "    \n",
    "    # Axis labels\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12, fontweight='300', labelpad=8)\n",
    "    ax.set_ylabel('True Label', fontsize=12, fontweight='300', labelpad=8)\n",
    "    \n",
    "    # Tick labels\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha='center', fontsize=10)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, va='center', fontsize=10)\n",
    "    \n",
    "    # Colorbar styling\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=9)\n",
    "    cbar.set_label('Count', fontsize=11, fontweight='300')\n",
    "\n",
    "# --------------------------------------------------\n",
    "# DistilBERT confusion matrices\n",
    "# --------------------------------------------------\n",
    "y_pred_full = model_predictions[\"DistilBERT_Full\"]\n",
    "y_pred_lora = model_predictions[\"DistilBERT_LoRA\"]\n",
    "\n",
    "cm_full = confusion_matrix(y_true, y_pred_full)\n",
    "cm_lora = confusion_matrix(y_true, y_pred_lora)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('DistilBERT Fine-Tuning Comparison', \n",
    "             fontsize=20, fontweight='500', y=1.01)\n",
    "\n",
    "plot_clean_confusion_matrix(cm_full, axes[0], \n",
    "                           'Full Fine-Tuning', cmap='Blues')\n",
    "plot_clean_confusion_matrix(cm_lora, axes[1], \n",
    "                           'LoRA Fine-Tuning', cmap='Oranges')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.savefig(results_dir / \"distilbert_confusion_enhanced.png\", \n",
    "            dpi=600, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# RoBERTa confusion matrices\n",
    "# --------------------------------------------------\n",
    "y_pred_full = model_predictions[\"RoBERTa_Full\"]\n",
    "y_pred_lora = model_predictions[\"RoBERTa_LoRA\"]\n",
    "\n",
    "cm_full = confusion_matrix(y_true, y_pred_full)\n",
    "cm_lora = confusion_matrix(y_true, y_pred_lora)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('RoBERTa Fine-Tuning Comparison', \n",
    "             fontsize=20, fontweight='500', y=1.01)\n",
    "\n",
    "plot_clean_confusion_matrix(cm_full, axes[0], \n",
    "                           'Full Fine-Tuning', cmap='Blues')\n",
    "plot_clean_confusion_matrix(cm_lora, axes[1], \n",
    "                           'LoRA Fine-Tuning', cmap='Oranges')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.savefig(results_dir / \"roberta_confusion_enhanced.png\", \n",
    "            dpi=600, bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbfd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot confusion matrix for the ensemble model and summarize error patterns.\n",
    "\n",
    "This cell:\n",
    "- Generates and saves the ensemble confusion matrix with clean styling\n",
    "- Computes per-class error rates for qualitative analysis\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "label_order = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "results_dir = Path(\"results/confusion_matrices\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set clean style\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "\n",
    "# Ensemble confusion matrix\n",
    "cm_ensemble = confusion_matrix(y_true, model_predictions[\"Ensemble\"])\n",
    "\n",
    "# Calculate percentages\n",
    "cm_percent = cm_ensemble.astype('float') / cm_ensemble.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Create annotations combining counts and percentages\n",
    "annotations = np.empty_like(cm_ensemble, dtype=object)\n",
    "for i in range(cm_ensemble.shape[0]):\n",
    "    for j in range(cm_ensemble.shape[1]):\n",
    "        count = cm_ensemble[i, j]\n",
    "        percent = cm_percent[i, j]\n",
    "        annotations[i, j] = f'{count}\\n({percent:.1f}%)'\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "sns.heatmap(\n",
    "    cm_ensemble,\n",
    "    annot=annotations,\n",
    "    fmt='',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_order,\n",
    "    yticklabels=label_order,\n",
    "    ax=ax,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    linewidths=1.5,\n",
    "    linecolor='white',\n",
    "    square=True,\n",
    "    vmin=0,\n",
    "    annot_kws={'fontsize': 12, 'weight': 'bold'}\n",
    ")\n",
    "\n",
    "# Title and labels\n",
    "ax.set_title('Confusion Matrix - Ensemble Model', \n",
    "             fontsize=20, fontweight='500', pad=15, y=1.01)\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='300', labelpad=10)\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='300', labelpad=10)\n",
    "\n",
    "# Tick labels\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha='center', fontsize=11)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, va='center', fontsize=11)\n",
    "\n",
    "# Colorbar styling\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "cbar.set_label('Count', fontsize=11, fontweight='300')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.savefig(results_dir / \"ensemble_confusion_enhanced.png\", \n",
    "            dpi=600, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Per-class error analysis\n",
    "# --------------------------------------------------\n",
    "ensemble_errors = pd.DataFrame(\n",
    "    cm_ensemble,\n",
    "    index=label_order,\n",
    "    columns=label_order\n",
    ")\n",
    "\n",
    "# Calculate error rates\n",
    "error_rates = 1 - np.diag(cm_ensemble) / cm_ensemble.sum(axis=1)\n",
    "error_summary = pd.Series(error_rates, index=label_order, name=\"Error Rate\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENSEMBLE MODEL - ERROR ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nOverall Accuracy: {(np.diag(cm_ensemble).sum() / cm_ensemble.sum()):.2%}\")\n",
    "print(\"\\nPer-Class Error Rates:\")\n",
    "print(\"-\"*50)\n",
    "for label in label_order:\n",
    "    error_rate = error_summary[label]\n",
    "    accuracy = 1 - error_rate\n",
    "    print(f\"{label:12s}: {error_rate:6.2%} error  |  {accuracy:6.2%} accuracy\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\n Ensemble confusion matrix saved to:\")\n",
    "print(f\"  {results_dir / 'ensemble_confusion_enhanced.png'}\")\n",
    "\n",
    "# Return error summary for further analysis\n",
    "error_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa94d67",
   "metadata": {},
   "source": [
    "### 10.4 Misclassification Case Analysis\n",
    "\n",
    "This subsection performs a qualitative analysis of misclassified test examples\n",
    "across all fine-tuned models and the ensemble. The objective is to identify\n",
    "systematic error patterns, ambiguous cases, and domain overlaps that contribute\n",
    "to incorrect predictions.\n",
    "\n",
    "For each model, a subset of misclassified examples is extracted along with\n",
    "the true label and predicted label. These cases are then organised into tables\n",
    "to support structured comparison and discussion in the written report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f856f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract misclassified examples for all fine-tuned models and the ensemble.\n",
    "\n",
    "This cell:\n",
    "- Reloads all fine-tuned models explicitly\n",
    "- Generates predictions on the test set\n",
    "- Extracts misclassified samples for each model\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "label_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "y_true = np.array(mini_dataset[\"test\"][\"label\"])\n",
    "\n",
    "def predict_labels(model, dataset, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate predicted labels for a dataset.\n",
    "    \"\"\"\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            logits = model(\n",
    "                input_ids=batch[\"input_ids\"].to(device),\n",
    "                attention_mask=batch[\"attention_mask\"].to(device)\n",
    "            ).logits\n",
    "            preds.append(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "\n",
    "    return np.concatenate(preds)\n",
    "\n",
    "# Explicit model registry\n",
    "model_registry = {\n",
    "    \"DistilBERT_Full\": {\n",
    "        \"type\": \"full\",\n",
    "        \"path\": \"models/distilbert_full/best_model\",\n",
    "        \"base\": \"distilbert-base-uncased\",\n",
    "        \"dataset\": \"distilbert\",\n",
    "    },\n",
    "    \"DistilBERT_LoRA\": {\n",
    "        \"type\": \"lora\",\n",
    "        \"path\": \"models/distilbert_lora/best_model\",\n",
    "        \"base\": \"distilbert-base-uncased\",\n",
    "        \"dataset\": \"distilbert\",\n",
    "    },\n",
    "    \"RoBERTa_Full\": {\n",
    "        \"type\": \"full\",\n",
    "        \"path\": \"models/roberta_full/best_model\",\n",
    "        \"base\": \"roberta-base\",\n",
    "        \"dataset\": \"roberta\",\n",
    "    },\n",
    "    \"RoBERTa_LoRA\": {\n",
    "        \"type\": \"lora\",\n",
    "        \"path\": \"models/roberta_lora/best_model\",\n",
    "        \"base\": \"roberta-base\",\n",
    "        \"dataset\": \"roberta\",\n",
    "    },\n",
    "}\n",
    "\n",
    "misclassified_rows = []\n",
    "\n",
    "for model_name, cfg in model_registry.items():\n",
    "\n",
    "    # Load model\n",
    "    if cfg[\"type\"] == \"lora\":\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            cfg[\"base\"], num_labels=4\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base_model, cfg[\"path\"])\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(cfg[\"path\"])\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = predict_labels(\n",
    "        model,\n",
    "        tokenized_datasets[cfg[\"dataset\"]][\"test\"]\n",
    "    )\n",
    "\n",
    "    # Collect misclassifications\n",
    "    for idx, (t, p) in enumerate(zip(y_true, y_pred)):\n",
    "        if t != p:\n",
    "            misclassified_rows.append({\n",
    "                \"model\": model_name,\n",
    "                \"text\": mini_dataset[\"test\"][idx][\"text\"],\n",
    "                \"true_label\": label_names[t],\n",
    "                \"predicted_label\": label_names[p],\n",
    "            })\n",
    "\n",
    "# Add ensemble misclassifications\n",
    "ensemble_preds = model_predictions[\"Ensemble\"]\n",
    "\n",
    "for idx, (t, p) in enumerate(zip(y_true, ensemble_preds)):\n",
    "    if t != p:\n",
    "        misclassified_rows.append({\n",
    "            \"model\": \"Ensemble\",\n",
    "            \"text\": mini_dataset[\"test\"][idx][\"text\"],\n",
    "            \"true_label\": label_names[t],\n",
    "            \"predicted_label\": label_names[p],\n",
    "        })\n",
    "\n",
    "misclassified_df = pd.DataFrame(misclassified_rows)\n",
    "\n",
    "misclassified_df[\"model\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa11d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Select representative misclassification examples for qualitative analysis.\n",
    "\n",
    "This cell:\n",
    "- Creates one table per model\n",
    "- Samples a fixed number of misclassified cases for each model\n",
    "- Stores results in a dictionary of DataFrames\n",
    "\"\"\"\n",
    "\n",
    "# Number of examples per model to inspect\n",
    "N_SAMPLES = 10\n",
    "\n",
    "sampled_errors_per_model = {}\n",
    "\n",
    "for model_name in misclassified_df[\"model\"].unique():\n",
    "    model_errors = misclassified_df[misclassified_df[\"model\"] == model_name]\n",
    "\n",
    "    sampled_errors = model_errors.sample(\n",
    "        n=min(N_SAMPLES, len(model_errors)),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    sampled_errors_per_model[model_name] = sampled_errors\n",
    "\n",
    "    print(f\"\\nMisclassified examples for {model_name}:\")\n",
    "    display(sampled_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524c698",
   "metadata": {},
   "source": [
    "### 10.5 Comparative Error Patterns and Observations\n",
    "\n",
    "This subsection analyses misclassification patterns across all fine-tuned models\n",
    "and the ensemble to identify systematic errors and model-specific behaviour.\n",
    "Rather than focusing on individual examples, the analysis aggregates errors to\n",
    "reveal consistent class confusions, overlap in failure cases, and the extent to\n",
    "which the ensemble mitigates errors made by single models.\n",
    "\n",
    "The results in this subsection support higher-level observations about model\n",
    "biases, domain overlap between news categories, and the benefits and limitations\n",
    "of ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Aggregate class-to-class confusion patterns for each model.\n",
    "\n",
    "This cell:\n",
    "- Computes normalised confusion matrices\n",
    "- Converts them into long-form tables\n",
    "- Highlights dominant misclassification directions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "label_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "per_model_confusions = {}\n",
    "\n",
    "for model_name, preds in model_predictions.items():\n",
    "    cm = confusion_matrix(\n",
    "        y_true,\n",
    "        preds,\n",
    "        labels=range(len(label_names))\n",
    "    )\n",
    "\n",
    "    # Normalise by true class counts\n",
    "    cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    rows = []\n",
    "    for i, true_label in enumerate(label_names):\n",
    "        for j, pred_label in enumerate(label_names):\n",
    "            if i != j:\n",
    "                rows.append({\n",
    "                    \"true_label\": true_label,\n",
    "                    \"predicted_label\": pred_label,\n",
    "                    \"error_rate\": cm_norm[i, j]\n",
    "                })\n",
    "\n",
    "    df = (\n",
    "        pd.DataFrame(rows)\n",
    "        .sort_values(\"error_rate\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    per_model_confusions[model_name] = df\n",
    "\n",
    "# Display tables separately for each model\n",
    "for model_name, df in per_model_confusions.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43137b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analyse overlap in misclassified samples between models.\n",
    "\n",
    "This cell:\n",
    "- Computes which test samples are misclassified by each model\n",
    "- Quantifies shared and unique error cases\n",
    "- Evaluates whether the ensemble reduces shared errors\n",
    "\"\"\"\n",
    "\n",
    "error_flags = {}\n",
    "\n",
    "for model_name, preds in model_predictions.items():\n",
    "    error_flags[model_name] = (preds != y_true)\n",
    "\n",
    "error_flag_df = pd.DataFrame(error_flags)\n",
    "\n",
    "# Count number of models misclassifying each sample\n",
    "error_flag_df[\"num_models_misclassified\"] = error_flag_df.sum(axis=1)\n",
    "\n",
    "# Summary statistics\n",
    "overlap_summary = (\n",
    "    error_flag_df[\"num_models_misclassified\"]\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .rename(\"num_samples\")\n",
    ")\n",
    "\n",
    "display(\n",
    "    overlap_summary\n",
    "    .rename_axis(\"Number of models that misclassified the sample\")\n",
    "    .reset_index(name=\"Number of test samples\")\n",
    "    .style.hide(axis=\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf67bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assess how often the ensemble corrects errors made by individual models.\n",
    "\n",
    "This cell:\n",
    "- Identifies samples misclassified by single models\n",
    "- Checks whether the ensemble predicts them correctly\n",
    "- Quantifies corrective impact of ensembling\n",
    "\"\"\"\n",
    "\n",
    "ensemble_correct = model_predictions[\"Ensemble\"] == y_true\n",
    "\n",
    "correction_stats = []\n",
    "\n",
    "for model_name in model_predictions:\n",
    "    if model_name == \"Ensemble\":\n",
    "        continue\n",
    "\n",
    "    model_wrong = model_predictions[model_name] != y_true\n",
    "    corrected_by_ensemble = model_wrong & ensemble_correct\n",
    "\n",
    "    correction_stats.append({\n",
    "        \"model\": model_name,\n",
    "        \"errors_by_model\": model_wrong.sum(),\n",
    "        \"errors_corrected_by_ensemble\": corrected_by_ensemble.sum(),\n",
    "        \"percent_corrected\": corrected_by_ensemble.sum() / model_wrong.sum()\n",
    "        if model_wrong.sum() > 0 else 0.0\n",
    "    })\n",
    "\n",
    "ensemble_correction_df = pd.DataFrame(correction_stats)\n",
    "\n",
    "ensemble_correction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61781b7",
   "metadata": {},
   "source": [
    "## 11. Training Dynamics, Efficiency, and Representation Analysis\n",
    "\n",
    "This section analyses how different fine-tuning strategies behave during training,\n",
    "how computationally efficient they are, and how learned representations differ\n",
    "across models. The analysis is divided into four parts:\n",
    "\n",
    "1. Training loss curves  \n",
    "2. Evaluation loss curves  \n",
    "3. Training efficiency comparison  \n",
    "4. Embedding space visualisation  \n",
    "\n",
    "Together, these analyses provide deeper insight into model convergence,\n",
    "generalisation behaviour, efficiency trade-offs, and representational learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot training loss curves for all fine-tuned models.\n",
    "\n",
    "This cell:\n",
    "- Loads Trainer log history from disk\n",
    "- Extracts training loss per epoch\n",
    "- Plots loss curves for comparison\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def load_training_loss(model_dir):\n",
    "    \"\"\"\n",
    "    Load training loss history from a Trainer checkpoint directory.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): Path to model directory.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple]: (epoch, loss) pairs.\n",
    "    \"\"\"\n",
    "    state_path = Path(model_dir) / \"trainer_state.json\"\n",
    "\n",
    "    with open(state_path, \"r\") as f:\n",
    "        state = json.load(f)\n",
    "\n",
    "    return [\n",
    "        (log[\"epoch\"], log[\"loss\"])\n",
    "        for log in state[\"log_history\"]\n",
    "        if \"loss\" in log and \"epoch\" in log\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Model paths\n",
    "# --------------------------------------------------\n",
    "model_dirs = {\n",
    "    \"DistilBERT Full\": \"models/distilbert_full/checkpoint-1000\",\n",
    "    \"DistilBERT LoRA\": \"models/distilbert_lora/checkpoint-1000\",\n",
    "    \"RoBERTa Full\": \"models/roberta_full/checkpoint-1000\",\n",
    "    \"RoBERTa LoRA\": \"models/roberta_lora/checkpoint-1000\",\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Figure with two grids\n",
    "# --------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "# DistilBERT panel\n",
    "for name, style in {\n",
    "    \"DistilBERT Full\": {\"color\": \"#DE8F05\", \"linestyle\": \"-\", \"marker\": \"o\"},\n",
    "    \"DistilBERT LoRA\": {\"color\": \"#0173B2\", \"linestyle\": \"--\", \"marker\": \"s\"},\n",
    "}.items():\n",
    "    epochs, losses = zip(*load_training_loss(model_dirs[name]))\n",
    "    axes[0].plot(epochs, losses, label=name, linewidth=2.5, markersize=7, **style)\n",
    "\n",
    "axes[0].set_title(\"DistilBERT Training Loss\", fontsize=15, fontweight=\"500\", y=1.05)\n",
    "axes[0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Training Loss\", fontsize=12)\n",
    "axes[0].grid(True, linestyle=\"--\", alpha=0.3)\n",
    "axes[0].tick_params(axis=\"y\", labelleft=True)\n",
    "axes[0].legend()\n",
    "\n",
    "# RoBERTa panel\n",
    "for name, style in {\n",
    "   \"RoBERTa Full\": {\"color\": \"#DE8F05\", \"linestyle\": \"-\", \"marker\": \"o\"},\n",
    "   \"RoBERTa LoRA\": {\"color\": \"#0173B2\", \"linestyle\": \"--\", \"marker\": \"s\"},\n",
    "}.items():\n",
    "    epochs, losses = zip(*load_training_loss(model_dirs[name]))\n",
    "    axes[1].plot(epochs, losses, label=name, linewidth=2.5, markersize=7, **style)\n",
    "\n",
    "axes[1].set_title(\"RoBERTa Training Loss\", fontsize=15, fontweight=\"500\", y=1.05)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Training Loss\", fontsize=12)\n",
    "axes[1].grid(True, linestyle=\"--\", alpha=0.3)\n",
    "axes[1].tick_params(axis=\"y\", labelleft=True)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c3df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot evaluation loss curves for all fine-tuned models.\n",
    "\n",
    "This cell:\n",
    "- Loads Trainer evaluation logs from disk\n",
    "- Extracts evaluation loss per epoch\n",
    "- Plots evaluation loss curves for comparison\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def load_eval_loss(model_dir):\n",
    "    \"\"\"\n",
    "    Load evaluation loss history from a Trainer checkpoint directory.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str): Path to model directory.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple]: (epoch, eval_loss) pairs.\n",
    "    \"\"\"\n",
    "    state_path = Path(model_dir) / \"trainer_state.json\"\n",
    "\n",
    "    with open(state_path, \"r\") as f:\n",
    "        state = json.load(f)\n",
    "\n",
    "    return [\n",
    "        (log[\"epoch\"], log[\"eval_loss\"])\n",
    "        for log in state[\"log_history\"]\n",
    "        if \"eval_loss\" in log and \"epoch\" in log\n",
    "    ]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Model paths\n",
    "# --------------------------------------------------\n",
    "model_dirs = {\n",
    "    \"DistilBERT Full\": \"models/distilbert_full/checkpoint-1000\",\n",
    "    \"DistilBERT LoRA\": \"models/distilbert_lora/checkpoint-1000\",\n",
    "    \"RoBERTa Full\": \"models/roberta_full/checkpoint-1000\",\n",
    "    \"RoBERTa LoRA\": \"models/roberta_lora/checkpoint-1000\",\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Figure with two grids\n",
    "# --------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "# DistilBERT panel\n",
    "for name, style in {\n",
    "    \"DistilBERT Full\": {\"color\": \"#DE8F05\", \"linestyle\": \"-\", \"marker\": \"o\"},\n",
    "    \"DistilBERT LoRA\": {\"color\": \"#0173B2\", \"linestyle\": \"--\", \"marker\": \"s\"},\n",
    "}.items():\n",
    "    epochs, losses = zip(*load_eval_loss(model_dirs[name]))\n",
    "    axes[0].plot(epochs, losses, label=name, linewidth=2.5, markersize=7, **style)\n",
    "\n",
    "axes[0].set_title(\"DistilBERT Evaluation Loss\", fontsize=15, fontweight=\"500\", y=1.05)\n",
    "axes[0].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Evaluation Loss\", fontsize=12)\n",
    "axes[0].grid(True, linestyle=\"--\", alpha=0.3)\n",
    "axes[0].tick_params(axis=\"y\", labelleft=True)\n",
    "axes[0].legend()\n",
    "\n",
    "# RoBERTa panel\n",
    "for name, style in {\n",
    "   \"RoBERTa Full\": {\"color\": \"#DE8F05\", \"linestyle\": \"-\", \"marker\": \"o\"},\n",
    "   \"RoBERTa LoRA\": {\"color\": \"#0173B2\", \"linestyle\": \"--\", \"marker\": \"s\"},\n",
    "}.items():\n",
    "    epochs, losses = zip(*load_eval_loss(model_dirs[name]))\n",
    "    axes[1].plot(epochs, losses, label=name, linewidth=2.5, markersize=7, **style)\n",
    "\n",
    "axes[1].set_title(\"RoBERTa Evaluation Loss\", fontsize=15, fontweight=\"500\", y=1.05)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Training Loss\", fontsize=12)\n",
    "axes[1].grid(True, linestyle=\"--\", alpha=0.3)\n",
    "axes[1].tick_params(axis=\"y\", labelleft=True)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compare training efficiency across fine-tuning strategies.\n",
    "\n",
    "This cell:\n",
    "- Loads saved training metrics\n",
    "- Compares runtime, throughput, and FLOPs\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "metric_files = {\n",
    "    \"DistilBERT Full\": \"models/distilbert_full/train_metrics.json\",\n",
    "    \"DistilBERT LoRA\": \"models/distilbert_lora/train_metrics.json\",\n",
    "    \"RoBERTa Full\": \"models/roberta_full/train_metrics.json\",\n",
    "    \"RoBERTa LoRA\": \"models/roberta_lora/train_metrics.json\",\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for name, path in metric_files.items():\n",
    "    with open(path, \"r\") as f:\n",
    "        m = json.load(f)\n",
    "\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"Train Runtime (s)\": m.get(\"train_runtime\"),\n",
    "        \"Samples / Second\": m.get(\"train_samples_per_second\"),\n",
    "        \"Total FLOPs\": m.get(\"total_flos\"),\n",
    "        \"Train Loss\": m.get(\"train_loss\"),\n",
    "    })\n",
    "\n",
    "efficiency_df = pd.DataFrame(rows).set_index(\"Model\")\n",
    "efficiency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb487f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Embedding space comparison for DistilBERT using PCA.\n",
    "\n",
    "This cell:\n",
    "- Extracts CLS embeddings for DistilBERT Baseline, Full Fine-Tuning, and LoRA\n",
    "- Fits PCA once on the baseline embeddings\n",
    "- Projects all embeddings into the same PCA space\n",
    "- Plots three subplots for direct comparison\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "baseline_distilbert = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=4\n",
    ").to(device)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper: extract CLS embeddings\n",
    "# --------------------------------------------------\n",
    "def extract_embeddings(model, dataset, max_samples=400):\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset.select(range(max_samples)),\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"].to(device),\n",
    "                attention_mask=batch[\"attention_mask\"].to(device),\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            cls = outputs.hidden_states[-1][:, 0, :].cpu().numpy()\n",
    "            embeddings.append(cls)\n",
    "            labels.extend(batch[\"label\"].numpy())\n",
    "\n",
    "    return np.vstack(embeddings), np.array(labels)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Extract embeddings\n",
    "# --------------------------------------------------\n",
    "emb_base, y = extract_embeddings(\n",
    "    baseline_distilbert,\n",
    "    tokenized_datasets[\"distilbert\"][\"test\"]\n",
    ")\n",
    "\n",
    "emb_full, _ = extract_embeddings(\n",
    "    loaded_models[\"DistilBERT_Full\"],\n",
    "    tokenized_datasets[\"distilbert\"][\"test\"]\n",
    ")\n",
    "\n",
    "emb_lora, _ = extract_embeddings(\n",
    "    loaded_models[\"DistilBERT_LoRA\"],\n",
    "    tokenized_datasets[\"distilbert\"][\"test\"]\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Fit PCA on baseline embeddings only\n",
    "# --------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "emb_base_scaled = scaler.fit_transform(emb_base)\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "emb_base_2d = pca.fit_transform(emb_base_scaled)\n",
    "\n",
    "# Project others into same PCA space\n",
    "emb_full_2d = pca.transform(scaler.transform(emb_full))\n",
    "emb_lora_2d = pca.transform(scaler.transform(emb_lora))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Plot styling\n",
    "# --------------------------------------------------\n",
    "colors = [\"#0173B2\", \"#DE8F05\", \"#CC78BC\", \"#CA9161\"]\n",
    "cmap = plt.cm.colors.ListedColormap(colors)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6.5), sharex=True, sharey=True)\n",
    "fig.patch.set_facecolor(\"#ffffff\")\n",
    "\n",
    "titles = [\n",
    "    \"Baseline (No Fine-Tuning)\",\n",
    "    \"Full Fine-Tuning\",\n",
    "    \"LoRA Fine-Tuning\"\n",
    "]\n",
    "\n",
    "data = [emb_base_2d, emb_full_2d, emb_lora_2d]\n",
    "\n",
    "for ax, emb_2d, title in zip(axes, data, titles):\n",
    "    ax.scatter(\n",
    "        emb_2d[:, 0], emb_2d[:, 1],\n",
    "        c=y, cmap=cmap, s=100, alpha=0.7,\n",
    "        edgecolors=\"white\", linewidth=0.8,\n",
    "        rasterized=True\n",
    "    )\n",
    "    \n",
    "    ax.set_title(title, fontsize=18, fontweight=\"300\", pad=15)\n",
    "    ax.set_xlabel(\"PCA Component 1\", fontsize=16, fontweight=\"150\", labelpad=14)\n",
    "    ax.set_ylabel(\"PCA Component 2\", fontsize=16, fontweight=\"150\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.2, linewidth=0.8)\n",
    "    ax.tick_params(axis=\"y\", labelleft=True)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(\"#e0e0e0\")\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Variance explained annotation (global)\n",
    "# --------------------------------------------------\n",
    "var_pc1 = pca.explained_variance_ratio_[0] * 100\n",
    "var_pc2 = pca.explained_variance_ratio_[1] * 100\n",
    "\n",
    "# Main title\n",
    "fig.suptitle(\n",
    "    \"DistilBERT Embedding Space Comparison (PCA)\",\n",
    "    fontsize=20, fontweight=\"500\", y=1.07\n",
    ")\n",
    "\n",
    "# Subtitle (variance explained)\n",
    "fig.text(\n",
    "    0.5, 0.98,\n",
    "    f\"PC1: {var_pc1:.1f}% variance  |   PC2: {var_pc2:.1f}% variance\",\n",
    "    ha=\"center\", fontsize=18, fontweight=\"150\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7606cfc0",
   "metadata": {},
   "source": [
    "#### for report\n",
    "Figure X illustrates the evolution of the DistilBERT embedding space under different training strategies. In the baseline model, embeddings form a compact and largely unstructured cluster, reflecting the absence of task-specific representation learning. After full fine-tuning, the embedding space becomes elongated and structured, indicating that the model has learned discriminative directions relevant to news category classification. LoRA fine-tuning produces an intermediate geometry: embeddings are more dispersed than the baseline, demonstrating task adaptation, but less structured than full fine-tuning, consistent with its slightly lower classification performance. These results suggest that while LoRA captures much of the task-specific structure, full fine-tuning more strongly reshapes the latent representation space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d4b949",
   "metadata": {},
   "source": [
    "## 12. Deployment of the Ensemble Model\n",
    "\n",
    "This section deploys the ensemble-based news classification system as an\n",
    "interactive web interface using Gradio. The deployed system allows users to\n",
    "input a news headline or short description and receive a predicted category\n",
    "along with class probability scores.\n",
    "\n",
    "The ensemble combines multiple fine-tuned transformer models at the probability\n",
    "level, leveraging their complementary strengths to produce more robust and\n",
    "reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843fb379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load fine-tuned models for ensemble deployment.\n",
    "\n",
    "This cell:\n",
    "- Loads the best fine-tuned checkpoints for all models\n",
    "- Prepares them for inference\n",
    "- Sets models to evaluation mode\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Label mapping\n",
    "label_names = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "\n",
    "# Model configuration\n",
    "deployment_models = {\n",
    "    \"DistilBERT_Full\": {\n",
    "        \"path\": \"models/distilbert_full/best_model\",\n",
    "        \"base\": \"distilbert-base-uncased\",\n",
    "        \"lora\": False,\n",
    "    },\n",
    "    \"DistilBERT_LoRA\": {\n",
    "        \"path\": \"models/distilbert_lora/best_model\",\n",
    "        \"base\": \"distilbert-base-uncased\",\n",
    "        \"lora\": True,\n",
    "    },\n",
    "    \"RoBERTa_Full\": {\n",
    "        \"path\": \"models/roberta_full/best_model\",\n",
    "        \"base\": \"roberta-base\",\n",
    "        \"lora\": False,\n",
    "    },\n",
    "    \"RoBERTa_LoRA\": {\n",
    "        \"path\": \"models/roberta_lora/best_model\",\n",
    "        \"base\": \"roberta-base\",\n",
    "        \"lora\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "loaded_deployment_models = {}\n",
    "loaded_tokenizers = {}\n",
    "\n",
    "for name, cfg in deployment_models.items():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg[\"base\"])\n",
    "    loaded_tokenizers[name] = tokenizer\n",
    "\n",
    "    if cfg[\"lora\"]:\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            cfg[\"base\"], num_labels=4\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base_model, cfg[\"path\"])\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(cfg[\"path\"])\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    loaded_deployment_models[name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Flexible prediction function supporting ensemble and single-model inference.\n",
    "\n",
    "This cell:\n",
    "- Loads the best ensemble configuration from disk\n",
    "- Supports ensemble-based or single-model predictions\n",
    "- Uses a unified inference pipeline for both modes\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Load best ensemble configuration\n",
    "with open(\"results/best_ensemble.json\", \"r\") as f:\n",
    "    best_ensemble = json.load(f)\n",
    "\n",
    "ensemble_model_names = best_ensemble[\"models\"]\n",
    "ensemble_weights = np.array(best_ensemble[\"weights\"], dtype=float)\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "def predict(text, mode=\"ensemble\", model_name=None):\n",
    "    \"\"\"\n",
    "    Predict the news category using either an ensemble or a single model.\n",
    "\n",
    "    Args:\n",
    "        text (str): News headline or short description.\n",
    "        mode (str): Prediction mode (\"ensemble\" or \"single\").\n",
    "        model_name (str, optional): Model key to use in single-model mode.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Predicted label and dictionary of class probabilities.\n",
    "    \"\"\"\n",
    "    if mode == \"ensemble\":\n",
    "        model_names = ensemble_model_names\n",
    "        weights = ensemble_weights\n",
    "\n",
    "    elif mode == \"single\":\n",
    "        if model_name is None:\n",
    "            raise ValueError(\n",
    "                \"model_name must be specified when mode='single'.\"\n",
    "            )\n",
    "        model_names = [model_name]\n",
    "        weights = np.array([1.0])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be either 'ensemble' or 'single'.\")\n",
    "\n",
    "    all_probs = []\n",
    "\n",
    "    for name in model_names:\n",
    "        model = loaded_deployment_models[name]\n",
    "        tokenizer = loaded_tokenizers[name]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LENGTH\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "        all_probs.append(probs)\n",
    "\n",
    "    # Aggregate probabilities\n",
    "    combined_probs = np.zeros_like(all_probs[0])\n",
    "    for w, p in zip(weights, all_probs):\n",
    "        combined_probs += w * p\n",
    "\n",
    "    predicted_index = int(np.argmax(combined_probs))\n",
    "\n",
    "    return (\n",
    "        label_names[predicted_index],\n",
    "        {label: float(prob) for label, prob in zip(label_names, combined_probs)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create and launch an interactive Gradio interface for model inference.\n",
    "\n",
    "This interface:\n",
    "- Allows users to select the model (single or ensemble) via UI controls\n",
    "- Accepts a news headline or short description as input\n",
    "- Displays class probabilities for all four categories\n",
    "- Provides text-only examples without coupling to model selection\n",
    "\"\"\"\n",
    "\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_MAP = {\n",
    "    \"Ensemble\": {\"mode\": \"ensemble\", \"model_name\": None},\n",
    "    \"RoBERTa (Full)\": {\"mode\": \"single\", \"model_name\": \"RoBERTa_Full\"},\n",
    "    \"RoBERTa (LoRA)\": {\"mode\": \"single\", \"model_name\": \"RoBERTa_LoRA\"},\n",
    "    \"DistilBERT (Full)\": {\"mode\": \"single\", \"model_name\": \"DistilBERT_Full\"},\n",
    "    \"DistilBERT (LoRA)\": {\"mode\": \"single\", \"model_name\": \"DistilBERT_LoRA\"},\n",
    "}\n",
    "\n",
    "def predict_category(text, model_choice):\n",
    "    config = MODEL_MAP[model_choice]\n",
    "    _, probabilities = predict(\n",
    "        text,\n",
    "        mode=config[\"mode\"],\n",
    "        model_name=config[\"model_name\"]\n",
    "    )\n",
    "    return probabilities\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Example table (2 columns, 5 rows)\n",
    "# --------------------------------------------------\n",
    "example_texts = [\n",
    "    \"Google unveils new AI chip for cloud customers.\",\n",
    "    \"European Union holds emergency meeting on rising tensions.\",\n",
    "    \"Scientists discover new exoplanet in nearby solar system.\",\n",
    "    \"Tech companies, governments, and sports organizations meet to discuss new economic reforms.\",\n",
    "    \"Officials raise concerns about rapid growth following recent announcements.\",\n",
    "    \"European regulators investigate major tech firms for violating new digital privacy rules.\",\n",
    "    \"Top athletes launch crypto startup focused on fan engagement tokens.\",\n",
    "    \"Government to nationalize struggling telecom company after years of financial losses.\",\n",
    "    \"NFL signs multi-billion dollar broadcast deal with major streaming platform.\",\n",
    "    \"Serena Williams confirms her return for the upcoming Grand Slam tournament.\",\n",
    "]\n",
    "\n",
    "example_df = pd.DataFrame({\n",
    "    \"Example A\": example_texts[:5],\n",
    "    \"Example B\": example_texts[5:]\n",
    "})\n",
    "\n",
    "def load_example(evt: gr.SelectData):\n",
    "    return example_df.iloc[evt.index[0], evt.index[1]]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# UI\n",
    "# --------------------------------------------------\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h2 style='text-align: center;'>News Article Classification Demo</h2>\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            model_choice = gr.Radio(\n",
    "                choices=list(MODEL_MAP.keys()),\n",
    "                value=\"Ensemble\",\n",
    "                label=\"Model Selection\"\n",
    "            )\n",
    "\n",
    "            text_input = gr.Textbox(\n",
    "                lines=3,\n",
    "                placeholder=\"Enter a news headline or short description\",\n",
    "                label=\"News Text\"\n",
    "            )\n",
    "\n",
    "            submit = gr.Button(\"Submit\")\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            output = gr.Label(\n",
    "                num_top_classes=4,\n",
    "                label=\"Predicted Category\"\n",
    "            )\n",
    "\n",
    "    gr.Markdown(\"### Example News Headlines\")\n",
    "\n",
    "    example_table = gr.Dataframe(\n",
    "        value=example_df,\n",
    "        interactive=False,\n",
    "        wrap=True\n",
    "    )\n",
    "\n",
    "    example_table.select(\n",
    "        fn=load_example,\n",
    "        outputs=text_input\n",
    "    )\n",
    "\n",
    "    submit.click(\n",
    "        fn=predict_category,\n",
    "        inputs=[text_input, model_choice],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbb354",
   "metadata": {},
   "source": [
    "#### Deploying on HuggingFace for Public Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eceacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# Clear any existing token\n",
    "os.environ.pop(\"HF_TOKEN\", None)\n",
    "\n",
    "# Load .env from project root\n",
    "project_root = Path.cwd().parent\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "# Verify\n",
    "print(\"HF_TOKEN loaded:\", \"HF_TOKEN\" in os.environ)\n",
    "\n",
    "# Authenticate\n",
    "login(token=os.environ[\"HF_TOKEN\"])\n",
    "HfApi().whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92822ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repos = [\n",
    "    \"news-distilbert-full\",\n",
    "    \"news-distilbert-lora\",\n",
    "    \"news-roberta-full\",\n",
    "    \"news-roberta-lora\",\n",
    "]\n",
    "\n",
    "for repo in repos:\n",
    "    create_repo(\n",
    "        repo_id=f\"heezuss/{repo}\",\n",
    "        exist_ok=True,\n",
    "        private=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Upload fine-tuned models to the Hugging Face Model Hub.\n",
    "\n",
    "This cell:\n",
    "- Uploads full fine-tuned models (DistilBERT, RoBERTa)\n",
    "- Uploads LoRA adapter models (DistilBERT, RoBERTa)\n",
    "- Uploads corresponding tokenizers\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "HF_USERNAME = \"heezuss\" \n",
    "\n",
    "models_to_upload = {\n",
    "    \"news-distilbert-full\": {\n",
    "        \"type\": \"full\",\n",
    "        \"local_path\": \"models/distilbert_full/best_model\",\n",
    "        \"base\": \"distilbert-base-uncased\",\n",
    "    },\n",
    "    \"news-distilbert-lora\": {\n",
    "        \"type\": \"lora\",\n",
    "        \"local_path\": \"models/distilbert_lora/best_model\",\n",
    "        \"base\": \"distilbert-base-uncased\",\n",
    "    },\n",
    "    \"news-roberta-full\": {\n",
    "        \"type\": \"full\",\n",
    "        \"local_path\": \"models/roberta_full/best_model\",\n",
    "        \"base\": \"roberta-base\",\n",
    "    },\n",
    "    \"news-roberta-lora\": {\n",
    "        \"type\": \"lora\",\n",
    "        \"local_path\": \"models/roberta_lora/best_model\",\n",
    "        \"base\": \"roberta-base\",\n",
    "    },\n",
    "}\n",
    "\n",
    "for repo_name, cfg in models_to_upload.items():\n",
    "    repo_id = f\"{HF_USERNAME}/{repo_name}\"\n",
    "    print(f\"\\nUploading to {repo_id}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg[\"base\"])\n",
    "\n",
    "    if cfg[\"type\"] == \"full\":\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            cfg[\"local_path\"]\n",
    "        )\n",
    "    else:\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            cfg[\"base\"], num_labels=4\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            base_model, cfg[\"local_path\"]\n",
    "        )\n",
    "\n",
    "    model.push_to_hub(repo_id)\n",
    "    tokenizer.push_to_hub(repo_id)\n",
    "\n",
    "print(\"\\nAll models successfully uploaded to Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create and upload README.md (model card) for each model repository.\n",
    "\"\"\"\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "README_CONTENT = \"\"\"# News Article Classification Model\n",
    "\n",
    "Fine-tuned transformer model for classifying news articles into four categories:\n",
    "World, Sports, Business, and Sci/Tech.\n",
    "\n",
    "**Dataset:** AG News  \n",
    "**Fine-tuning:** Supervised (Full fine-tuning or LoRA-based fine-tuning)  \n",
    "**Maximum sequence length:** 128  \n",
    "\n",
    "This model was trained and evaluated as part of an academic project on applied\n",
    "natural language processing and transformer-based text classification.\n",
    "\"\"\"\n",
    "\n",
    "for repo_name in models_to_upload.keys():\n",
    "    repo_id = f\"{HF_USERNAME}/{repo_name}\"\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=README_CONTENT.encode(\"utf-8\"),\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "\n",
    "print(\"README.md added to all model repositories.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
